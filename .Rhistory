# (1)
gen_data <-function(dgp,prob_G){
Ndim = dgp@Ndim
Tdim = dgp@Tdim
G = dgp@G
siga = dgp@sig_alpha
sigerr = dgp@sig_err
alpha <- sort(rnorm(G,0,siga))
error <- matrix(rnorm(Ndim*Tdim,0,sigerr),Ndim,Tdim)
assign <- sample(sample(1:G,Ndim,replace=TRUE,prob=prob_G))
alpha_mat <- matrix(rep(alpha[assign],Tdim),Ndim,Tdim)
y = alpha_mat + error
return(list(y,alpha,assign))
}
# (2) Lloyd's algorithm
Lloyds <- function(dgp,y,alpha_ini,itermax){
Ndim <- dgp@Ndim
Tdim <- dgp@Tdim
G <- dgp@G
tol <- 0.0001
dif <- 1
data <- CJ(i=1:Ndim,t=1:Tdim)
data[,y:=as.vector(t(y))]
res = matrix(0,Ndim,G)
alpha_new = rep(0,G)
iter = 1
while(dif > tol &&  iter < 50){
# assignment
for(gg in 1:G){
resaux = y -  matrix(rep(alpha_ini[gg],Ndim*Tdim),Ndim,Tdim)
resaux2 = resaux^2
res[,gg] = rowSums(resaux2)
}
new_assign = apply(res, 1, FUN = which.min)
data[,assign:=as.character(new_assign[i])]
# update alpha
model <- glm(formula = y ~ assign + 0, data = data)
dif = 0
for(gg in 1:G){
alpha_new[gg] = model$coefficients[[gg]]
dif = dif +  (alpha_new[gg] - alpha_ini[gg])^2
}
#print(dif)
alpha_ini = alpha_new
iter = iter+1
#print(iter)
}
data[,alpha_out:=alpha_ini[as.numeric(assign)]]
loss = sum((data$y-data$alpha_out)^2)
return(list(data,loss))
}
setClass("DGP", slots = list(Ndim = "numeric",Tdim = "numeric",G = "numeric",sig_alpha = "numeric", sig_err = "numeric"))
dgp <- new("DGP",Ndim=1000,Tdim=10,G=3,sig_alpha = 2,sig_err=1)
prob_G = c(0.1,0.2,0.7)
itermax = 50
# Generate data:
data_out <- gen_data(dgp,prob_G)
y <- data_out[[1]]
alpha0 <- data_out[[2]]
assign0 <- data_out[[3]]
# Estimate the model
ini_cond = 10
ii = 1
alpha_ini = alpha0 + rnorm(3,0,1)
output <- Lloyds(dgp,y,alpha_ini)
dataout <- output[[1]]
require(data.table)
output <- Lloyds(dgp,y,alpha_ini)
dataout <- output[[1]]
loss <- output[[2]]/(Ndim*Tdim)
if(ii ==1){
min_loss = loss
best_data = dataout
}else{
if(loss < min_loss){
min_loss = loss
best_data = data_out
# Compute missclassification probability:
missclass <- 1-sum(best_data$assign[seq(1, Ndim*Tdim, by = Tdim)] == assign0)/Ndim
print(list(missclass,loss))
}
}
loss <- output[[2]]/(Ndim*Tdim)
loss <- output[[2]]/(dgp@Ndim*dgp@Tdim)
if(ii ==1){
min_loss = loss
best_data = dataout
}else{
if(loss < min_loss){
min_loss = loss
best_data = data_out
# Compute missclassification probability:
missclass <- 1-sum(best_data$assign[seq(1, Ndim*Tdim, by = Tdim)] == assign0)/Ndim
print(list(missclass,loss))
}
}
missclass <- 1-sum(best_data$assign[seq(1, Ndim*Tdim, by = Tdim)] == assign0)/Ndim
for(ii in 1:ini_cond){
alpha_ini = alpha0 + rnorm(3,0,1)
output <- Lloyds(dgp,y,alpha_ini)
dataout <- output[[1]]
loss <- output[[2]]/(dgp@Ndim*dgp@Tdim)
if(ii ==1){
min_loss = loss
best_data = dataout
missclass <- 1-sum(best_data$assign[seq(1, dgp@Ndim*dgp@Tdim, by = dgp@Tdim)] == assign0)/dgp@Ndim
print(list(missclass,loss))
}else{
if(loss < min_loss){
min_loss = loss
best_data = data_out
# Compute missclassification probability:
missclass <- 1-sum(best_data$assign[seq(1, dgp@Ndim*dgp@Tdim, by = dgp@Tdim)] == assign0)/dgp@Ndim
print(list(missclass,loss))
}
}
}
for(ii in 1:ini_cond){
print(ii)
alpha_ini = alpha0 + rnorm(3,0,1)
output <- Lloyds(dgp,y,alpha_ini)
dataout <- output[[1]]
loss <- output[[2]]/(dgp@Ndim*dgp@Tdim)
if(ii ==1){
min_loss = loss
best_data = dataout
missclass <- 1-sum(best_data$assign[seq(1, dgp@Ndim*dgp@Tdim, by = dgp@Tdim)] == assign0)/dgp@Ndim
print(list(missclass,loss))
}else{
if(loss < min_loss){
min_loss = loss
best_data = data_out
# Compute missclassification probability:
missclass <- 1-sum(best_data$assign[seq(1, dgp@Ndim*dgp@Tdim, by = dgp@Tdim)] == assign0)/dgp@Ndim
print(list(missclass,loss))
}
}
}
View(dataout)
source('~/GitHub/cemfi_course/K_means.R', echo=TRUE)
source('~/GitHub/cemfi_course/K_means.R', echo=TRUE)
loss <- output[[2]]
loss
missclass <- 1-sum(dataout$assign[seq(1, dgp@Ndim*dgp@Tdim, by = dgp@Tdim)] == assign0)/dgp@Ndim
misclass
missclass
print(list(missclass,loss))
# Estimate the model
ini_cond = 10
for(ii in 1:ini_cond){
print(ii)
alpha_ini = rnorm(3,0,1)
output <- Lloyds(dgp,y,alpha_ini)
dataout <- output[[1]]
loss <- output[[2]]
missclass <- 1-sum(dataout$assign[seq(1, dgp@Ndim*dgp@Tdim, by = dgp@Tdim)] == assign0)/dgp@Ndim
print(list(missclass,loss))
if(ii ==1){
min_loss = loss
best_data = dataout
missclass <- 1-sum(best_data$assign[seq(1, dgp@Ndim*dgp@Tdim, by = dgp@Tdim)] == assign0)/dgp@Ndim
print(list(missclass,loss))
}else{
if(loss < min_loss){
min_loss = loss
best_data = data_out
# Compute missclassification probability:
}
}
}
levels(assign0)
uniue(assign0)
unique(assign0)
quantile(y,0.25)
quantile(y,)
quantile(y,G)
quantile(y,seq(0:1:G))
quantile(y,seq(0,1,G))
quantile(y,seq(0,1,dg@G))
quantile(y,seq(0,1,dgp@G))
dgp@G
quantile(y,seq(0,1,by=dgp@G))
quantile(y,seq(0,100,by=dgp@G))
quantile(y,seq(0,100,by=dgp@G)/100)
quantile(y,seq(0,100,dgp@G)/100)
quantile(y,seq(0,1,1/dgp@G)/100)
quantile(y,seq(1/dgp@G,1,1/dgp@G)/100)
quantile(y,seq(1/dgp@G,1-1/dgp@G,1/dgp@G)/100)
quantile(y,seq(1/(dgp@G+1),1-1/(dgp@G+1),1/dgp@G)/100)
quantile(y,seq(1/(dgp@G+1),1-1/(dgp@G+1),1/(dgp@G+1)/100)
)
quantile(y,seq(1/(dgp@G+1),1-1/(dgp@G+1),1/(dgp@G+1)))
alpha_ini = quantile(y,seq(1/(dgp@G+1),1-1/(dgp@G+1),1/(dgp@G+1)))
output <- Lloyds(dgp,y,alpha_ini)
dataout <- output[[1]]
loss <- output[[2]]
missclass <- 1-sum(dataout$assign[seq(1, dgp@Ndim*dgp@Tdim, by = dgp@Tdim)] == assign0)/dgp@Ndim
print(list(missclass,loss))
if(ii ==1){
min_loss = loss
best_data = dataout
missclass <- 1-sum(best_data$assign[seq(1, dgp@Ndim*dgp@Tdim, by = dgp@Tdim)] == assign0)/dgp@Ndim
print(list(missclass,loss))
}else{
if(loss < min_loss){
min_loss = loss
best_data = data_out
# Compute missclassification probability:
}
}
for(ii in 1:ini_cond){
print(ii)
alpha_ini = quantile(y,seq(1/(dgp@G+1),1-1/(dgp@G+1),1/(dgp@G+1)))
output <- Lloyds(dgp,y,alpha_ini)
dataout <- output[[1]]
loss <- output[[2]]
missclass <- 1-sum(dataout$assign[seq(1, dgp@Ndim*dgp@Tdim, by = dgp@Tdim)] == assign0)/dgp@Ndim
print(list(missclass,loss))
if(ii ==1){
min_loss = loss
best_data = dataout
missclass <- 1-sum(best_data$assign[seq(1, dgp@Ndim*dgp@Tdim, by = dgp@Tdim)] == assign0)/dgp@Ndim
print(list(missclass,loss))
}else{
if(loss < min_loss){
min_loss = loss
best_data = data_out
# Compute missclassification probability:
}
}
}
source('~/GitHub/cemfi_course/K_means.R', echo=TRUE)
best
source('~/GitHub/cemfi_course/K_means.R', echo=TRUE)
source('~/GitHub/cemfi_course/K_means.R', echo=TRUE)
best
source('~/GitHub/cemfi_course/K_means.R', echo=TRUE)
source('~/GitHub/cemfi_course/K_means.R', echo=TRUE)
source('~/GitHub/cemfi_course/K_means.R', echo=TRUE)
best
source('~/GitHub/cemfi_course/K_means.R', echo=TRUE)
best
missclass <- 1-sum(best_data$assign[seq(1, dgp@Ndim*dgp@Tdim, by = dgp@Tdim)] == assign0)/dgp@Ndim
print(list(missclass,loss))
best_data
dataout
View(data_out)
output <- Lloyds(dgp,y,alpha_ini)
dataout <- output[[1]]
dataout
dataout
source('~/GitHub/cemfi_course/K_means.R', echo=TRUE)
data_out
require(data.table)
loss = sum((dataoutput$y-dataoutput$alpha_out)^2)
dataoutput <- Lloyds(dgp,y,alpha_ini)
source('~/GitHub/cemfi_course/K_means.R', echo=TRUE)
source('~/GitHub/cemfi_course/K_means.R', echo=TRUE)
source('~/GitHub/cemfi_course/K_means.R', echo=TRUE)
best
View(best_data)
debugSource('~/GitHub/cemfi_course/K_means.R', echo=TRUE)
debugSource('~/GitHub/cemfi_course/K_means.R', echo=TRUE)
debugSource('~/GitHub/cemfi_course/K_means.R', echo=TRUE)
dgp <- new("DGP",Ndim=1000,Tdim=10,G=3,sig_alpha = 10,sig_err=1)
prob_G = c(0.1,0.2,0.7)
itermax = 50
# Generate data:
data_out <- gen_data(dgp,prob_G)
y <- data_out[[1]]
alpha0 <- data_out[[2]]
assign0 <- data_out[[3]]
# Estimate the model
ini_cond = 10
clear()
clear(all)
require(data.table)
# (1)
gen_data <-function(dgp,prob_G){
Ndim = dgp@Ndim
Tdim = dgp@Tdim
G = dgp@G
siga = dgp@sig_alpha
sigerr = dgp@sig_err
alpha <- sort(rnorm(G,0,siga))
error <- matrix(rnorm(Ndim*Tdim,0,sigerr),Ndim,Tdim)
assign <- sample(sample(1:G,Ndim,replace=TRUE,prob=prob_G))
alpha_mat <- matrix(rep(alpha[assign],Tdim),Ndim,Tdim)
y = alpha_mat + error
return(list(y,alpha,assign))
}
# (2) Lloyd's algorithm
Lloyds <- function(dgp,y,alpha_ini,itermax){
Ndim <- dgp@Ndim
Tdim <- dgp@Tdim
G <- dgp@G
tol <- 0.0001
dif <- 1
data <- CJ(i=1:Ndim,t=1:Tdim)
data[,y:=as.vector(t(y))]
res = matrix(0,Ndim,G)
alpha_new = rep(0,G)
iter = 1
while(dif > tol &&  iter < 50){
# assignment
for(gg in 1:G){
resaux = y -  matrix(rep(alpha_ini[gg],Ndim*Tdim),Ndim,Tdim)
resaux2 = resaux^2
res[,gg] = rowSums(resaux2)
}
new_assign = apply(res, 1, FUN = which.min)
data[,assign:=as.character(new_assign[i])]
# update alpha
model <- glm(formula = y ~ assign + 0, data = data)
dif = 0
for(gg in 1:G){
alpha_new[gg] = model$coefficients[[gg]]
dif = dif +  (alpha_new[gg] - alpha_ini[gg])^2
}
#print(dif)
alpha_ini = alpha_new
iter = iter+1
#print(iter)
}
data[,alpha_out:=alpha_ini[as.numeric(assign)]]
return(data)
}
setClass("DGP", slots = list(Ndim = "numeric",Tdim = "numeric",G = "numeric",sig_alpha = "numeric", sig_err = "numeric"))
dgp <- new("DGP",Ndim=1000,Tdim=10,G=3,sig_alpha = 10,sig_err=1)
prob_G = c(0.3,0.3,0.4)
itermax = 50
# Generate data:
data_out <- gen_data(dgp,prob_G)
y <- data_out[[1]]
alpha0 <- data_out[[2]]
assign0 <- data_out[[3]]
# Estimate the model
ini_cond = 10
ii = 1
alpha_ini = quantile(y,seq(1/(dgp@G+1),1-1/(dgp@G+1),1/(dgp@G+1))) + rnorm(dgp@G)
dataoutput <- Lloyds(dgp,y,alpha_ini)
loss = sum((dataoutput$y-dataoutput$alpha_out)^2)
dataoutput
loss = sum((dataoutput$y-dataoutput$alpha_out)^2)
missclass <- 1-sum(dataoutput$assign[seq(1, dgp@Ndim*dgp@Tdim, by = dgp@Tdim)] == assign0)/dgp@Ndim
print(list(missclass,loss))
if(ii ==1){
min_loss = loss
best_data = dataout
missclass <- 1-sum(best_data$assign[seq(1, dgp@Ndim*dgp@Tdim, by = dgp@Tdim)] == assign0)/dgp@Ndim
print(list(missclass,loss))
best = ii
}else{
if(loss < min_loss){
min_loss = loss
best_data = data_out
# Compute missclassification probability:
best = ii
}
}
for(ii in 1:ini_cond){
print(ii)
alpha_ini = quantile(y,seq(1/(dgp@G+1),1-1/(dgp@G+1),1/(dgp@G+1))) + rnorm(dgp@G)
dataoutput <- Lloyds(dgp,y,alpha_ini)
loss = sum((dataoutput$y-dataoutput$alpha_out)^2)
missclass <- 1-sum(dataoutput$assign[seq(1, dgp@Ndim*dgp@Tdim, by = dgp@Tdim)] == assign0)/dgp@Ndim
print(list(missclass,loss))
if(ii ==1){
min_loss = loss
best_data = dataout
missclass <- 1-sum(best_data$assign[seq(1, dgp@Ndim*dgp@Tdim, by = dgp@Tdim)] == assign0)/dgp@Ndim
print(list(missclass,loss))
best = ii
}else{
if(loss < min_loss){
min_loss = loss
best_data = data_out
# Compute missclassification probability:
best = ii
}
}
}
debugSource('~/GitHub/cemfi_course/K_means.R', echo=TRUE)
best
debugSource('~/GitHub/cemfi_course/K_means.R', echo=TRUE)
source('C:/cemfi_course/K_means.R', echo=TRUE)
source('C:/cemfi_course/K_means.R', echo=TRUE)
source('C:/cemfi_course/K_means.R', echo=TRUE)
# plot loss
plot(min_loss_mat[min_loss_mat<1000])
require(data.table)
renv:init()
renv::init()
install.packages("renv")
require(renv)
renv::init()
renv::snapshot()
